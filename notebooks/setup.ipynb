{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parkrun_scraper_sdk import Country, Course, Result, Event, ParkrunDataExtractionOrchestrator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from upath import UPath\n",
    "\n",
    "def create_folder_structure(base_path: UPath):\n",
    "    \"\"\"Create the folder structure for Parquet file storage.\"\"\"\n",
    "    folders = [\n",
    "        'countries',\n",
    "        'courses',\n",
    "        'events',\n",
    "        'results',\n",
    "        'runners'\n",
    "    ]\n",
    "    \n",
    "    for folder in folders:\n",
    "        path = os.path.join(base_path, folder)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(f\"Created folder: {path}\")\n",
    "\n",
    "base_path = UPath(\"/home/nathanielramm/parkrun_data\")  # You can change this to your preferred base path\n",
    "create_folder_structure(base_path)\n",
    "print(\"Folder structure created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from upath import UPath\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_countries_parquet(countries, base_path: UPath):\n",
    "    \"\"\"Write countries data to a Parquet file.\"\"\"\n",
    "    table = pa.Table.from_pylist([country.to_dict() for country in countries])\n",
    "    output_path = os.path.join(base_path, \"countries\", \"countries.parquet\")\n",
    "    pq.write_table(table, output_path)\n",
    "    print(f\"Countries data written to {output_path}\")\n",
    "\n",
    "def write_courses_parquet(courses, base_path: UPath):\n",
    "    \"\"\"Write courses data to a Parquet file.\"\"\"\n",
    "    table = pa.Table.from_pylist([course.to_dict() for course in courses])\n",
    "    output_path = os.path.join(base_path, \"courses\", \"courses.parquet\")\n",
    "    pq.write_table(table, output_path)\n",
    "    print(f\"Courses data written to {output_path}\")\n",
    "\n",
    "def write_events_parquet(events, base_path: UPath, course_id: str, date: str):\n",
    "    \"\"\"Write events data to a Parquet file, partitioned by course_id.\"\"\"\n",
    "\n",
    "    if isinstance(course_id, Course):\n",
    "        course_id = course_id.id\n",
    "    if isinstance(course_id, int):\n",
    "        course_id = str(course_id)\n",
    "\n",
    "\n",
    "    table = pa.Table.from_pylist([event.to_dict() for event in events])\n",
    "    output_path = os.path.join(base_path, \"events\", course_id, f\"events_{date}.parquet\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    pq.write_table(table, output_path)\n",
    "    print(f\"Events data written to {output_path}\")\n",
    "\n",
    "def write_results_parquet(results, base_path: UPath, course_id: str, date: str):\n",
    "    \"\"\"Write results data to a Parquet file, partitioned by course_id.\"\"\"\n",
    "\n",
    "    if isinstance(course_id, Course):\n",
    "        course_id = course_id.id\n",
    "    if isinstance(course_id, int):\n",
    "        course_id = str(course_id)\n",
    "\n",
    "\n",
    "    table = pa.Table.from_pylist([result.to_dict() for result in results])\n",
    "    output_path = os.path.join(base_path, \"results\", course_id, f\"results_{date}.parquet\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    pq.write_table(table, output_path)\n",
    "    print(f\"Results data written to {output_path}\")\n",
    "\n",
    "def write_runners_parquet(runners, base_path: UPath):\n",
    "    \"\"\"Write runners data to Parquet files, partitioned by athlete_id.\"\"\"\n",
    "    for runner in runners:\n",
    "        table = pa.Table.from_pylist([runner.to_dict()])\n",
    "        athlete_id = str(runner.athlete_id)\n",
    "        output_path = os.path.join(base_path, \"runners\", athlete_id[:2], f\"{athlete_id}.parquet\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        pq.write_table(table, output_path)\n",
    "    print(f\"Runners data written to {base_path}/runners/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def get_processed_countries(base_path: UPath):\n",
    "    \"\"\"Get the list of processed country IDs.\"\"\"\n",
    "    file_path = os.path.join(base_path, \"countries\", \"countries.parquet\")\n",
    "    if os.path.exists(file_path):\n",
    "        table = pq.read_table(file_path)\n",
    "        return table['id'].to_pylist()\n",
    "    return []\n",
    "\n",
    "def get_processed_courses(base_path: UPath):\n",
    "    \"\"\"Get the list of processed course IDs.\"\"\"\n",
    "    file_path = os.path.join(base_path, \"courses\", \"courses.parquet\")\n",
    "    if os.path.exists(file_path):\n",
    "        table = pq.read_table(file_path)\n",
    "        return table['id'].to_pylist()\n",
    "    return []\n",
    "\n",
    "def get_processed_events(base_path: UPath, course_id: str, date: str):\n",
    "    \"\"\"Get the list of processed event IDs for a specific course and date.\"\"\"\n",
    "\n",
    "    if isinstance(course_id, Course):\n",
    "        course_id = course_id.id\n",
    "    if isinstance(course_id, int):\n",
    "        course_id = str(course_id)\n",
    "\n",
    "\n",
    "    file_path = os.path.join(base_path, \"events\", course_id, f\"events_{date}.parquet\")\n",
    "    if os.path.exists(file_path):\n",
    "        table = pq.read_table(file_path)\n",
    "        return table['event_number'].to_pylist()\n",
    "    return []\n",
    "\n",
    "def get_processed_results(base_path: UPath, course_id: str, date: str):\n",
    "    \"\"\"Check if results for a specific course and date have been processed.\"\"\"\n",
    "\n",
    "    if isinstance(course_id, Course):\n",
    "        course_id = course_id.id\n",
    "    if isinstance(course_id, int):\n",
    "        course_id = str(course_id)\n",
    "\n",
    "\n",
    "    file_path = os.path.join(base_path, \"results\", course_id, f\"results_{date}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "def get_processed_runner(base_path: UPath, athlete_id):\n",
    "    \"\"\"Check if a runner with the given athlete_id has been processed.\"\"\"\n",
    "    file_path = os.path.join(base_path, \"runners\", athlete_id[:2], f\"{athlete_id}.parquet\")\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "\n",
    "def get_latest_processing_date(base_path: UPath, course_id: str):\n",
    "\n",
    "    if isinstance(course_id, Course):\n",
    "        course_id = course_id.id\n",
    "    if isinstance(course_id, int):\n",
    "        course_id = str(course_id)\n",
    "    \n",
    "    \"\"\"Get the latest processing date for a specific course.\"\"\"\n",
    "    events_path = os.path.join(base_path, \"events\", course_id)\n",
    "\n",
    "    if os.path.exists(events_path):\n",
    "        event_files = [f for f in os.listdir(events_path) if f.endswith('.parquet')]\n",
    "        if event_files:\n",
    "            latest_file = max(event_files)\n",
    "            return latest_file.split('_')[1].split('.')[0]  # Extract date from filename\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from parkrun_scraper_sdk import ParkrunDataExtractionOrchestrator\n",
    "# from parquet_writer import (\n",
    "#     write_countries_parquet, write_courses_parquet,\n",
    "#     write_events_parquet, write_results_parquet, write_runners_parquet\n",
    "# )\n",
    "# from file_checker import (\n",
    "#     get_processed_countries, get_processed_courses,\n",
    "#     get_processed_events, get_processed_results,\n",
    "#     get_processed_runner, get_latest_processing_date\n",
    "# )\n",
    "\n",
    "def main(base_path: UPath, country_ids=None, course_ids=None, processing_date: str=\"2014-01-01\", max_events_per_run=10000):\n",
    "\n",
    "    # base_path = \"parkrun_data\"\n",
    "    # processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Initialize the ParkrunDataExtractionOrchestrator\n",
    "    orchestrator = ParkrunDataExtractionOrchestrator(processing_date)\n",
    "    \n",
    "    # Process countries\n",
    "    countries = orchestrator.extract_raw_countries()\n",
    "    if country_ids:\n",
    "        countries = [country for country in countries if country.id in country_ids]\n",
    "    processed_countries = get_processed_countries(base_path)\n",
    "    new_countries = [country for country in countries if country.id not in processed_countries]\n",
    "\n",
    "    if new_countries:\n",
    "        write_countries_parquet(new_countries, base_path)\n",
    "    \n",
    "    # Process courses\n",
    "    courses = orchestrator.extract_raw_courses()\n",
    "    if course_ids:\n",
    "        courses = [course for course in courses if course.id in course_ids]\n",
    "    processed_courses = get_processed_courses(base_path)\n",
    "    new_courses = [course for course in courses if course.id not in processed_courses]\n",
    "    if new_courses:\n",
    "        write_courses_parquet(new_courses, base_path)\n",
    "    \n",
    "    # Process events and results\n",
    "    total_events_processed = 0\n",
    "    \n",
    "    for course in courses:\n",
    "\n",
    "        if total_events_processed >= max_events_per_run:\n",
    "            break\n",
    "        \n",
    "        latest_processing_date = get_latest_processing_date(base_path, course.id)\n",
    "        \n",
    "        start_date = datetime.strptime(latest_processing_date, \"%Y-%m-%d\") + timedelta(days=1) if latest_processing_date else None\n",
    "        end_date = datetime.strptime(processing_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        current_date = start_date or end_date\n",
    "        \n",
    "        while current_date <= end_date and total_events_processed < max_events_per_run:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            processed_events = get_processed_events(base_path, course.id, date_str)\n",
    "            new_events = orchestrator.extract_course_new_event_history(course.id, processed_events)\n",
    "            \n",
    "            if new_events:\n",
    "                events_to_write = list(new_events.values())[:max_events_per_run - total_events_processed]\n",
    "                write_events_parquet(events_to_write, base_path, course.id, date_str)\n",
    "                total_events_processed += len(events_to_write)\n",
    "            \n",
    "            for event_id, event in list(new_events.items())[:max_events_per_run - total_events_processed]:\n",
    "                if not get_processed_results(base_path, course.id, date_str):\n",
    "                    results = orchestrator.extract_raw_event_results(course.id, event_id)\n",
    "                    write_results_parquet(results, base_path, course.id, date_str)\n",
    "                    \n",
    "                    # # Process runners\n",
    "                    # for result in results:\n",
    "                    #     if not get_processed_runner(base_path, result.athlete_id):\n",
    "                    #         runner = orchestrator.extract_runner(result.athlete_id)\n",
    "                    #         write_runners_parquet([runner], base_path)\n",
    "            \n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "    print(f\"Processed {total_events_processed} events in this run.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage:\n",
    "#     # Process all countries and courses, with a limit of 1000 events per run\n",
    "#     main()\n",
    "\n",
    "    # Process specific countries and courses, with a limit of 500 events per run\n",
    "    # main(country_ids=['1', '2', '3'], course_ids=['10', '20', '30'], max_events_per_run=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = ParkrunDataExtractionOrchestrator( \"2024-10-22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.country_num_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(base_path, \"54\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from msilib import schema\n",
    "from cmath import polar\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from tkinter import E\n",
    "from typing import List, Dict, Any, Union\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from upath import UPath\n",
    "import polars\n",
    "\n",
    "from parkrun_scraper_sdk import Country, Course, Result, Event, ParkrunDataExtractionOrchestrator\n",
    "\n",
    "\n",
    "class BaseParquetHandler(ABC):\n",
    "\n",
    "    def __init__(self, base_path: UPath):\n",
    "        self.base_path = base_path\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def domain_folder(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def file_name_template(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def partition_keys(self) -> List[str]:\n",
    "        pass\n",
    "\n",
    "    def get_file_path(self, **kwargs) -> UPath:\n",
    "        \n",
    "        #add the partition keys to the file path. Preserve order!\n",
    "        ordered_partitions = []\n",
    "        for key in self.partition_keys:\n",
    "            if key in kwargs:\n",
    "                ordered_partitions.append(f\"{key}={kwargs[key]}\")        \n",
    "\n",
    "        partition_path = '/'.join( ordered_partitions )       \n",
    "\n",
    "        file_name = self.file_name_template.format(**kwargs)\n",
    "        file_path =  os.path.join(self.base_path, self.domain_folder, partition_path, file_name)\n",
    "\n",
    "        return UPath(file_path)\n",
    "\n",
    "\n",
    "\n",
    "    def read_parquet(self, **kwargs) -> polars.LazyFrame:\n",
    "\n",
    "        file_path: UPath = self.get_file_path(**kwargs)\n",
    "\n",
    "        try:\n",
    "            return polars.scan_parquet(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_processed_ids(self, id_column: str, **kwargs) -> List[str]:\n",
    "\n",
    "        table: polars.LazyFrame = self.read_parquet(**kwargs)#.select(polars.col(id_column))\n",
    "\n",
    "        if table is not None:\n",
    "\n",
    "            try:\n",
    "                table_dicts =  table.collect().to_dict()\n",
    "                col = table_dicts[id_column]\n",
    "                return [str(item) for item in col]\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting processed IDs: {e}\")\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "    def file_exists(self, **kwargs) -> bool:\n",
    "\n",
    "        file_path: UPath = self.get_file_path(**kwargs)\n",
    "        return file_path.exists()\n",
    "    \n",
    "    def write_parquet(self, data: List[Dict[str, Any]], **kwargs):\n",
    "\n",
    "        items = [item.to_dict() for item in data]\n",
    "        table = pa.Table.from_pylist(items)\n",
    "\n",
    "        output_path = self.get_file_path(**kwargs)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        pq.write_table(table, output_path)\n",
    "        \n",
    "        print(f\"Data written to {output_path}\")\n",
    "\n",
    "\n",
    "class CountriesHandler(BaseParquetHandler):\n",
    "    domain_folder = \"countries\"\n",
    "    # file_name_template = \"countries_{processing_date}.parquet\"\n",
    "    file_name_template = \"countries.parquet\"\n",
    "    partition_keys = []\n",
    "    \n",
    "\n",
    "\n",
    "    def get_stored_countries(self) -> List[str]:\n",
    "        return self.get_processed_ids('country_id')\n",
    "\n",
    "class CoursesHandler(BaseParquetHandler):\n",
    "\n",
    "    domain_folder = \"courses\"\n",
    "    # file_name_template = \"courses_{processing_date}.parquet\"\n",
    "    file_name_template = \"courses.parquet\"\n",
    "    partition_keys = []\n",
    "\n",
    "    def get_stored_courses(self) -> List[str]:\n",
    "        return self.get_processed_ids('course_id')\n",
    "\n",
    "class EventsHandler(BaseParquetHandler):\n",
    "\n",
    "    domain_folder = \"events\"\n",
    "    file_name_template = \"events_{course_id}.parquet\"\n",
    "    partition_keys = ['country_id']\n",
    "\n",
    "    def get_processed_event_ids(self, course_id: str, country_id: str) -> List[str]:\n",
    "        \n",
    "        return self.get_processed_ids('event_id', course_id=course_id, country_id=country_id)\n",
    "\n",
    "    # def get_latest_processing_date(self, course_id: str) -> str:\n",
    "\n",
    "    #     events_path = os.path.join(self.base_path, self.domain_folder, f\"course_id={course_id}\")\n",
    "\n",
    "    #     if os.path.exists(events_path):\n",
    "\n",
    "    #         event_files = [f for f in os.listdir(events_path) if f.endswith('.parquet')]\n",
    "            \n",
    "    #         if event_files:\n",
    "    #             latest_file = max(event_files)\n",
    "    #             return latest_file.split('_')[1].split('.')[0]  # Extract date from filename\n",
    "            \n",
    "    #     return None\n",
    "\n",
    "class ResultsHandler(BaseParquetHandler):\n",
    "    domain_folder = \"results\"\n",
    "    file_name_template = \"results_{course_id}_{event_id}.parquet\"\n",
    "    partition_keys = ['country_id', 'course_id']\n",
    "\n",
    "    def get_processed_result_event_ids(self, course_id: str, country_id: str ) -> List[str]:\n",
    "\n",
    "        event_ids =  self.get_processed_ids('event_id', course_id=course_id, country_id=country_id, event_id=\"*\")\n",
    "        \n",
    "        return list(set(event_ids))\n",
    "\n",
    "\n",
    "class RunnersHandler(BaseParquetHandler):\n",
    "    domain_folder = \"runners\"\n",
    "    file_name_template = \"{athlete_id}.parquet\"\n",
    "    partition_keys = ['athlete_id']\n",
    "\n",
    "    def get_processed_runner(self, athlete_id: str) -> bool:\n",
    "        return self.file_exists(athlete_id=athlete_id)\n",
    "\n",
    "# Usage example:\n",
    "base_path = UPath(\"/home/nathanielramm/parkrun_data\")\n",
    "countries_handler = CountriesHandler(base_path)\n",
    "courses_handler = CoursesHandler(base_path)\n",
    "events_handler = EventsHandler(base_path)\n",
    "results_handler = ResultsHandler(base_path)\n",
    "runners_handler = RunnersHandler(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from parkrun_scraper_sdk import ParkrunDataExtractionOrchestrator, course\n",
    "import concurrent.futures\n",
    "from typing import List\n",
    "\n",
    "# from domain_handlers import (\n",
    "#     CountriesHandler, CoursesHandler, EventsHandler, ResultsHandler, RunnersHandler\n",
    "# )\n",
    "\n",
    "# def process_event(orchestrator, course, event):\n",
    "#     event_result = orchestrator.extract_raw_event_results(course, event)\n",
    "#     results_handler.write_parquet(event_result, course_id=course.course_id, country_id=course.country_id, event_id=event.event_id)\n",
    "\n",
    "\n",
    "def do_it(base_path: UPath, processing_date: str = \"2014-01-01\", country_ids: List[str]=None, course_ids: List[str]=None):\n",
    "\n",
    "\n",
    "    # base_path = \"parkrun_data\"\n",
    "    # processing_date = datetime.strftime(\"%Y-%m-%d\")\n",
    "    # processing_date = datetime(2014, 1, 1).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #Preprocess the country_ids and course_ids\n",
    "    if country_ids is None:\n",
    "        country_ids = []\n",
    "    elif isinstance(country_ids, str):\n",
    "        country_ids = [country_ids]\n",
    "    elif isinstance(country_ids, int):\n",
    "        country_ids = [str(country_ids)]\n",
    "    country_ids = [str(country_id) for country_id in country_ids]\n",
    "\n",
    "    if course_ids is None:\n",
    "        course_ids = []\n",
    "    elif isinstance(course_ids, str):\n",
    "        course_ids = [course_ids]\n",
    "    elif isinstance(course_ids, int):\n",
    "        course_ids = [str(course_ids)]\n",
    "    course_ids = [str(course_id) for course_id in course_ids]\n",
    "\n",
    "\n",
    "    # Initialize handlers\n",
    "    countries_handler = CountriesHandler(base_path)\n",
    "    courses_handler =   CoursesHandler(base_path)\n",
    "    events_handler =    EventsHandler(base_path)\n",
    "    results_handler =   ResultsHandler(base_path)\n",
    "    runners_handler =   RunnersHandler(base_path)\n",
    "    \n",
    "    # Initialize the ParkrunDataExtractionOrchestrator\n",
    "    orchestrator = ParkrunDataExtractionOrchestrator(processing_date)\n",
    "\n",
    "    ########################################################    \n",
    "    # Process countries\n",
    "    ########################################################\n",
    "    live_country_ids =   list(orchestrator.live_countries_lookup.keys())\n",
    "    stored_country_ids = countries_handler.get_stored_countries()\n",
    "\n",
    "    if len(country_ids) > 0:\n",
    "        #If we are only processing specific countries, then don't update the stored countries\n",
    "\n",
    "        filtered_live_country_ids =   [country_id for country_id in live_country_ids  if country_id in country_ids]\n",
    "        unsupported_country_ids =     [country_id for country_id in country_ids       if country_id not in live_country_ids]\n",
    "        new_country_ids =             [country_id for country_id in filtered_live_country_ids if country_id not in stored_country_ids]\n",
    "\n",
    "        if len(unsupported_country_ids) > 0:\n",
    "            raise Exception(f\"You requested {unsupported_country_ids}: countries that are not supported by parkrun.\")\n",
    "\n",
    "        if len(new_country_ids) > 0:\n",
    "            raise Exception(f\"You requested {new_country_ids}: countries that have not been previously stored. Run a full update to store these countries.\")\n",
    "\n",
    "        country_ids_to_process = filtered_live_country_ids\n",
    "\n",
    "    if len(country_ids) == 0:\n",
    "        #No filtering, so update the stored countries if required!\n",
    "\n",
    "        new_country_ids = [country_id for country_id in live_country_ids if country_id not in stored_country_ids]\n",
    "\n",
    "        if len(new_country_ids) > 0:\n",
    "            print(f\"Identified {new_country_ids}: countries that have not been previously stored.\")\n",
    "\n",
    "            live_countries =   orchestrator.extract_raw_countries()\n",
    "            countries_handler.write_parquet(live_countries)\n",
    "\n",
    "        country_ids_to_process = live_country_ids\n",
    "\n",
    "\n",
    "    ########################################################    \n",
    "    # Process courses\n",
    "    ########################################################\n",
    "\n",
    "    live_course_ids =   list(orchestrator.live_courses_lookup.keys())\n",
    "    stored_course_ids = courses_handler.get_stored_courses()\n",
    "\n",
    "    if len(course_ids) > 0:\n",
    "        #If we are only processing specific courses, then don't update the stored courses\n",
    "\n",
    "        filtered_live_course_ids =   [course_id for course_id in live_course_ids            if course_id in course_ids]\n",
    "        unsupported_course_ids =     [course_id for course_id in course_ids                 if course_id not in live_course_ids]\n",
    "        new_course_ids =             [course_id for course_id in filtered_live_course_ids   if course_id not in stored_course_ids]\n",
    "\n",
    "        if len(unsupported_course_ids) > 0:\n",
    "            raise Exception(f\"You requested {unsupported_course_ids}: courses that are not supported by parkrun.\")\n",
    "\n",
    "        if len(new_course_ids) > 0:\n",
    "            raise Exception(f\"You requested {new_course_ids}: courses that have not been previously stored. Run a full update to store these courses.\")\n",
    "\n",
    "        course_ids_to_process = filtered_live_course_ids\n",
    "\n",
    "    if len(course_ids) == 0:\n",
    "        #No filtering, so update the stored courses if required!\n",
    "\n",
    "        new_course_ids = [course_id for course_id in live_course_ids if course_id not in stored_course_ids]\n",
    "\n",
    "        if len(new_course_ids) > 0:\n",
    "            print(f\"Identified {new_course_ids}: courses that have not been previously stored.\")\n",
    "\n",
    "            live_courses =   orchestrator.extract_raw_courses()\n",
    "            courses_handler.write_parquet(live_courses)\n",
    "\n",
    "        course_ids_to_process = live_course_ids\n",
    "\n",
    "\n",
    "    if len(country_ids_to_process) > 0:\n",
    "\n",
    "        course_ids_not_in_countries = [course_id for course_id in course_ids_to_process if orchestrator.live_courses_lookup[course_id].country_id not in country_ids_to_process]\n",
    "        if len(course_ids_not_in_countries) > 0:\n",
    "            print(f\"Excluding {course_ids_not_in_countries}: courses that are not in the specified countries.\")\n",
    "        \n",
    "        course_ids_to_process = [course_id for course_id in course_ids_to_process if orchestrator.live_courses_lookup[course_id].country_id in country_ids_to_process]\n",
    "\n",
    "    \n",
    "    print(f\"Processing {len(country_ids_to_process)} countries and {len(course_ids_to_process)} courses.\")\n",
    "    print(f\"Processing course_ids: {course_ids_to_process}\")\n",
    "\n",
    "    # Process events and results\n",
    "   \n",
    "    for course_id in course_ids_to_process:\n",
    "\n",
    "        course = orchestrator.live_courses_lookup[course_id]\n",
    "\n",
    "        ############################\n",
    "        # Events    \n",
    "        ############################\n",
    "        processed_event_ids = events_handler.get_processed_event_ids(course_id=course.course_id, country_id=course.country_id)\n",
    "\n",
    "        new_events = orchestrator.extract_course_new_event_history(course, processed_event_ids)\n",
    "        course_first_event_date = orchestrator.get_course_first_event_date(course)\n",
    "\n",
    "        if parser.parse(course_first_event_date).date() > parser.parse(processing_date).date():\n",
    "            print(f\"Course {course.course_id} has its first event on {course_first_event_date}. Processing is only being performed up to {processing_date}.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"processed_events: {processed_event_ids}\")\n",
    "        print(f\"new_events: {new_events}\")\n",
    "\n",
    "        #Update the event history file - a single file for each course\n",
    "        if new_events:\n",
    "\n",
    "            events_to_write = list(new_events.values())\n",
    "            print(f\"events_to_write: {events_to_write}\")\n",
    "            events_handler.write_parquet(data=events_to_write, course_id=course.course_id, country_id=course.country_id)\n",
    "\n",
    "        ############################        \n",
    "        # Results\n",
    "        ############################\n",
    "        \n",
    "        #Todo: Based on how many events there are to process, we should divide them up into smaller chunks to avoid memory issues\n",
    "        #Ideally paralleise this process\n",
    "\n",
    "\n",
    "        known_event_ids = events_handler.get_processed_event_ids(course_id=course.course_id, country_id=course.country_id)\n",
    "        processed_result_event_ids = results_handler.get_processed_result_event_ids(course_id=course.course_id, country_id = course.country_id)\n",
    "        fully_processed_events = [event_id for event_id in processed_event_ids if event_id in processed_result_event_ids]\n",
    "\n",
    "\n",
    "        print(f\"processed_result_event_ids: {processed_result_event_ids}\")\n",
    "\n",
    "        #Unprocessed results that are before the processing date. This gets all of them!\n",
    "        unprocessed_results: List[Event] = orchestrator.extract_course_unprocessed_result_events(course, fully_processed_events)\n",
    "\n",
    "        print(f\"unprocessed_results: {unprocessed_results}\")\n",
    "\n",
    "        # for event in unprocessed_results:\n",
    "        #     event_result = orchestrator.extract_raw_event_results(course, event)\n",
    "        #     results_handler.write_parquet(event_result, course_id=course.course_id, country_id=course.country_id, event_id=event.event_id)\n",
    "\n",
    "        #Parallelise the processing of the events\n",
    "        def process_event(course, event):\n",
    "            event_result = orchestrator.extract_raw_event_results(course, event)\n",
    "            results_handler.write_parquet(event_result, course_id=course.course_id, country_id=course.country_id, event_id=event.event_id)\n",
    "\n",
    "\n",
    "        unprocessed_results: List[Event] = orchestrator.extract_course_unprocessed_result_events(course, fully_processed_events)\n",
    "        print(f\"unprocessed_results: {unprocessed_results}\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_event, course, event) for event in unprocessed_results]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    future.result()  # To raise exceptions if any\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing event: {e}\")\n",
    "\n",
    "\n",
    "        # for event_id in unprocessed_results.keys():\n",
    "\n",
    "        #     results_handler.write_parquet(unprocessed_results[event_id], course_id=course.course_id, country_id=course.country_id, event_id=event_id)\n",
    "\n",
    "                    \n",
    "                    # # Process runners\n",
    "                    # new_runners = []\n",
    "                    # for result in results:\n",
    "                    #     if not runners_handler.get_processed_runner(result.athlete_id):\n",
    "                    #         runner = orchestrator.extract_runner(result.athlete_id)\n",
    "                    #         new_runners.append(runner)\n",
    "                    \n",
    "                    # if new_runners:\n",
    "                    #     for runner in new_runners:\n",
    "                    #         runners_handler.write_parquet([runner], athlete_id=runner.athlete_id)\n",
    "            \n",
    "            # current_date += timedelta(days=1)\n",
    "\n",
    "    # print(f\"Processed {total_events_processed} events in this run.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Process all countries and courses, with a limit of 1000 events per run\n",
    "    # main()\n",
    "\n",
    "    # Process specific countries and courses, with a limit of 500 events per run\n",
    "    # main(country_ids=['1', '2', '3'], course_ids=['10', '20', '30'], max_events_per_run=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 countries and 1 courses.\n",
      "Processing course_ids: ['2857']\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "405 Client Error: Not Allowed for url: https://www.parkrun.co.at/hellbrunn/results/eventhistory/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m processing_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-10-12\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdo_it\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcourse_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2857\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 146\u001b[0m, in \u001b[0;36mdo_it\u001b[0;34m(base_path, processing_date, country_ids, course_ids)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Events    \u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    144\u001b[0m processed_event_ids \u001b[38;5;241m=\u001b[39m events_handler\u001b[38;5;241m.\u001b[39mget_processed_event_ids(course_id\u001b[38;5;241m=\u001b[39mcourse\u001b[38;5;241m.\u001b[39mcourse_id, country_id\u001b[38;5;241m=\u001b[39mcourse\u001b[38;5;241m.\u001b[39mcountry_id)\n\u001b[0;32m--> 146\u001b[0m new_events \u001b[38;5;241m=\u001b[39m \u001b[43morchestrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_course_new_event_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcourse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_event_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m course_first_event_date \u001b[38;5;241m=\u001b[39m orchestrator\u001b[38;5;241m.\u001b[39mget_course_first_event_date(course)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse(course_first_event_date)\u001b[38;5;241m.\u001b[39mdate() \u001b[38;5;241m>\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(processing_date)\u001b[38;5;241m.\u001b[39mdate():\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/src/parkrun_scraper_sdk/orchestration/extraction_orchestrator.py:130\u001b[0m, in \u001b[0;36mParkrunDataExtractionOrchestrator.extract_course_new_event_history\u001b[0;34m(self, course, processed_event_ids)\u001b[0m\n\u001b[1;32m    127\u001b[0m     processed_event_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#This will get the current events for the course from the website\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_raw_course_event_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcourse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m event_id_lookup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_course_event_id_lookup(events)\n\u001b[1;32m    133\u001b[0m processing_date \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_date)\u001b[38;5;241m.\u001b[39mdate()\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/src/parkrun_scraper_sdk/orchestration/extraction_orchestrator.py:92\u001b[0m, in \u001b[0;36mParkrunDataExtractionOrchestrator.extract_raw_course_event_history\u001b[0;34m(self, course)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_raw_course_event_history\u001b[39m(\u001b[38;5;28mself\u001b[39m, course: Course) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Event]:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Implementation for extracting events for a specific course\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# course = self.live_courses_lookup[course_id]\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcourse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/src/parkrun_scraper_sdk/course.py:34\u001b[0m, in \u001b[0;36mCourse.get_event_history\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_event_history\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_history) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:            \n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_history \u001b[38;5;241m=\u001b[39m \u001b[43mEvent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_history\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcourse_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcourse_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountry_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_history\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/src/parkrun_scraper_sdk/event.py:56\u001b[0m, in \u001b[0;36mEvent.get_event_history\u001b[0;34m(cls, course_id, course_url, country_id)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_event_history\u001b[39m(\u001b[38;5;28mcls\u001b[39m, course_id: \u001b[38;5;28mstr\u001b[39m, course_url: \u001b[38;5;28mstr\u001b[39m, country_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     55\u001b[0m     url \u001b[38;5;241m=\u001b[39m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourse_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mresults/eventhistory/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m     html \u001b[38;5;241m=\u001b[39m          \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     soup \u001b[38;5;241m=\u001b[39m          \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_html(html)\n\u001b[1;32m     58\u001b[0m     history_rows \u001b[38;5;241m=\u001b[39m  soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr.Results-table-row\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/src/parkrun_scraper_sdk/base_scraper.py:13\u001b[0m, in \u001b[0;36mBaseScraper._fetch_data\u001b[0;34m(cls, url)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SessionManager\u001b[38;5;241m.\u001b[39mget_session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     12\u001b[0m     response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/git/parkrun-scraper-sdk/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 405 Client Error: Not Allowed for url: https://www.parkrun.co.at/hellbrunn/results/eventhistory/"
     ]
    }
   ],
   "source": [
    "processing_date = \"2024-10-12\"\n",
    "\n",
    "do_it(base_path, processing_date=processing_date, country_ids=[\"4\"], course_ids=[\"2857\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "res = ( polars.scan_parquet(\"/home/nathanielramm/parkrun_data/results/country_id=4/course_id=2857/results_2857_*.parquet\") \n",
    "        # .filter(pl.col(\"age_grade\") > \"80.00\" )\n",
    ")\n",
    "\n",
    "res.collect()#['event_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = res.collect()\n",
    "cols = d.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.read_table(\"/home/nathanielramm/parkrun_data/results/course_id=3549/results_3549_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parkrun_scraper_sdk import Country, Course, Result, Event, ParkrunDataExtractionOrchestrator\n",
    "\n",
    "from dataclasses import dataclass, asdict, fields\n",
    "import stat\n",
    "from typing import Any, Dict, get_origin, get_args, Optional, Type\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event().polars_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_origin, get_args, Union, Optional, List, Dict, Any\n",
    "from collections.abc import Sequence\n",
    "\n",
    "def get_first_concrete_type(type_hint: Any) -> type:\n",
    "    \"\"\"\n",
    "    Recursively explore type hints to find the first concrete type.\n",
    "    \n",
    "    :param type_hint: The type hint to explore\n",
    "    :return: The first concrete type found\n",
    "    \"\"\"\n",
    "    # If it's a simple type (int, str, etc.), return it directly\n",
    "    if isinstance(type_hint, type):\n",
    "        return type_hint\n",
    "    \n",
    "    # Get the origin of the type hint (e.g., Union, List, etc.)\n",
    "    origin = get_origin(type_hint)\n",
    "    \n",
    "    # If it's None (meaning it's a simple type), return the type hint itself\n",
    "    if origin is None:\n",
    "        return type_hint\n",
    "    \n",
    "    # Handle Union types (including Optional, which is Union[T, None])\n",
    "    if origin is Union:\n",
    "        args = get_args(type_hint)\n",
    "        # Filter out NoneType from Union args\n",
    "        concrete_args = [arg for arg in args if arg is not type(None)]\n",
    "        if concrete_args:\n",
    "            return get_first_concrete_type(concrete_args[0])\n",
    "    \n",
    "    # Handle List, Dict, and other generic types\n",
    "    elif issubclass(origin, (List, Sequence, Dict)):\n",
    "        args = get_args(type_hint)\n",
    "        if args:\n",
    "            return get_first_concrete_type(args[0])\n",
    "    \n",
    "    # For any other case, recurse into arguments\n",
    "    args = get_args(type_hint)\n",
    "    if args:\n",
    "        return get_first_concrete_type(args[0])\n",
    "    \n",
    "    # If we can't determine a concrete type, return Any\n",
    "    return Any\n",
    "\n",
    "# Test the function\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "def test_get_first_concrete_type():\n",
    "    assert get_first_concrete_type(int) == int\n",
    "    assert get_first_concrete_type(str) == str\n",
    "    assert get_first_concrete_type(Optional[int]) == int\n",
    "    assert get_first_concrete_type(Union[int, str]) == int\n",
    "    assert get_first_concrete_type(Optional[Union[int, str, list]]) == int\n",
    "    assert get_first_concrete_type(List[str]) == str\n",
    "    assert get_first_concrete_type(Dict[str, int]) == str\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_get_first_concrete_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.scan_parquet(\"/home/nathanielramm/parkrun_data/results/country_id=54/course_id=*/*.parquet\").collect()\n",
    "\n",
    "# x = t.filter(pl.col(\"athlete_id\") == 5215943)\n",
    "# x.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
